Tracking emotional responses during mock interviews presents a significant challenge, as traditional feedback relies on manual observation—often subjective, time-consuming, and prone to inconsistencies. To overcome these limitations, this system integrates facial expression analysis and deep learning to monitor candidates’ emotional states in real time.

Leveraging DeepFace for emotion detection and OpenCV for real-time video processing, the system captures and analyzes facial expressions during each interview question. It detects seven key emotions—happiness, sadness, anger, fear, surprise, disgust, and neutrality—from live webcam input, compiling this data into a comprehensive emotional profile of the candidate.

The system’s primary objective is to deliver a web-based emotional assessment platform that automatically detects, classifies, and visualizes emotional responses throughout mock interviews. To ensure reliability, facial input is limited to frontal, upright images with clear visibility, captured via the candidate’s webcam.
